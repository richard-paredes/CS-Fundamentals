HW3 Due 11:59PM April 16

***Please review the top part of ../hw1/hw1.txt***
***Your chance of success greatly increases if you start very early. Your chance of failure increases if you start late. Please use as many reviews as you possibly can.***

1.
According to the article <http://martinfowler.com/bliki/BeckDesignRules.html>, what are the rules of simple design. Why are these important? Discuss the impact of these principles. Give examples of using these principles in the class so far. Also give from examples outside of this class, but limit to your personal experiences.

Following Martin Fowler's interpretation, the rules of simple design, in order of priority, are to ensure the code passes tests, reveals intentions clearly, eliminates duplication, and incorporates only essential elements that preserve the prior three rules. Following the rules of simple design facilitate developers to produce higher quality software with better design. Specifically, each of the rules play a significant role in mitigating, or eliminating altogether, the accidental process of making software overly complex and unmaintainable.

By guaranteeing that the codebase passes all tests, developers are able to demonstrate that the software is useful in solving the intended problem for which it was written -- at least to the extent of the relevant coverage that the test cases provide. A design that fails to adhere to this rule is unable to provide consistent assurance to its clients and, in turn, is most likely brittle. Incorporating relevant tests to the software provides robustness and empowers developers to extend and refactor without fear, allowing the code to become more maintainable and evolvable. As the code is modified, the tests serve as a metric of verification which can alert developers of breaking changes. More importantly, as a user of the software, these tests serve as a declaration that the software is relevant and can be used to solve the problem at hand. Guided by Test-Driven Development, this rule has been paramount in every assignment in this class. For example, when reaching the stage of tactical design for every assignment, the implementation logic of every feature was driven by test cases. These tests provided an instant feedback process at every step of the way, verifying whether the specification requirements were satisfied prior to requesting code reviews. Through this iterative feedback cycle provided by TDD and the “passes all tests” rule, the design of every assignment remained robust and contained minimal complexity. 

(Richard) Yet beyond the scope of this course, the use of this rule has had a significant impact in my experience working as an intern software developer. Initially upon entering my project, there was extremely low code coverage which resulted in a feeling of fragile, unmaintainable code. Any major changes were risky, and refactoring was nearly impossible. But slowly, as I began incorporating tests when updating the legacy code and incorporating new features, the software began to feel more robust and issues were detected very quickly by our continuous integration process. Although there was a high initial overhead of incorporating more code coverage into the project, adding tests helped solidify my understanding of how the software was being used while also creating a safety net for future changes, ultimately making the entire process a valuable investment.

(Alan) Prior to the Software Design course, my experience with code coverage and testing was very limited. In my work as a data science intern, code coverage is often seen as an afterthought and not much thought is placed into testing code. Most of a data scientist’s effort is placed into developing a learning algorithm that will optimize a specific metric. This decision to avoid unit testing the codebase oftentimes becomes very problematic midway through a project as the requirements change regarding the data model or implementation of a learning algorithm which results in a huge refactoring effort and a hunt for obscure bugs that don’t necessarily “break the code” but produce illogical results in model predictions. After my experience with unit testing, code coverage and test-driven development in this course, I have extended my learnings into my internship and implemented unit tests as part of our continuous integration pipeline so that upon every push of new code we can run our unit tests to ensure that all tests pass and the new code development did not produce any failures or unexpected results.

Having code reveal its intentions plays a vital role in making sure that the software is easier to understand and maintainable. Developers have a natural habit of writing code that obfuscates its intentions. Consequently, the code is difficult to understand and debug by other developers, and many of its side-effects remain hidden, resulting in bugs. By putting the effort of writing code that explicitly declares its objective, developers have an easier time understanding how the code works and are able to quickly identify any potential issues or bugs. The end result is purposeful, cohesive software that reads like a good book. Conveyed by the code reviews for every assignment thus far, keeping the code minimal and easy to understand is a priority. For example, our assignments have incorporated radon to analyze the cyclomatic complexity and prevent the build from succeeding if complexity becomes too high. Particularly, in assignment 1, we found ourselves exploring various libraries, such as Python’s itertools, to reduce the complexity of the logic and make the code more declarative. This rule has pushed us to become better developers and think more creatively in how we approach software requirements. 

(Richard) Yet even when working with software outside of this class, I focus on making any code I write as readable as possible, often through the use of refactoring and creating separate classes or methods with discrete responsibilities. Building off my previous example as an intern, writing simple code with clear intentions has helped decouple many elements, allowing the code to be more testable. Without a doubt, finding ways to reduce the complexity can sometimes be challenging, but it is necessary to keep the code simple and intuitive.

(Alan) In my experience as an intern, I have had an opportunity to review many code repositories and experience the importance of this design rule. I have been fortunate to review great and beautiful code repositories that you can read like a story or a manual and immediately understand the purpose of each line of code. It is very evident that the software developers underwent several iterations to produce the concise and purposeful code in the repository. Likewise, I have also reviewed repositories that reflect minimal effort to reduce complexity or produce a codebase that can be readable and maintainable by others in the future. These are repositories that were selfishly developed to minimally meet the task at hand with no consideration for future developers. Therefore, after learning about such principles as the code revealing intention and first-hand witnessing the “beautiful” and the “ugly”, I put the extra effort to develop with intention and consideration for code readability and future maintenance.

Similarly to the prior rule, removing redundancy in code has great implications in the management and maintainability of software. When changes to the software are required, having duplication present necessitates changing every copy of the repeated code. This process creates room for mistakes, potentially leading to bugs, and also results in software that is more tedious to evolve. Through consolidating the duplication, the likelihood of bugs decreases as the efficiency of development increases. Instead of focusing efforts on identifying every place to make a repeated update, developers can redirect their attention to more fruitful changes to the software. In the context of our assignments so far, duplication is rejected in favor of reusability. A great example of how this rule was applied can be seen in the memoized recursive Fibonacci function of assignment 3. Rather than duplicating the recursion involved, we added extensibility to the base Fibonacci recursive function to allow its reuse in the memoized function. This way, there is a single point of truth for the logic and any updates can automatically cascade as needed. 

(Richard) In my other software projects, I have had the opportunity to exercise this rule many times, often through the use of inheritance. For example, I was once working on incorporating multiple learning policies to control the behavior of an autonomous agent for an artificial intelligence project. As I worked on adding these features, I realized all the learning policies shared common logic. Instead of repeating the same logic in every policy, I created a base policy class that contained the shared logic and created additional subclasses for each learning policy that each extended this logic. The end result was extremely simple and cohesive, and made it a painless process later when adding the feature to allow the autonomous agent to switch between policies.

(Alan) Continuing off with my experience as a data science intern, I have been witness to the experimentation practices by data scientists as they quickly iterate on several versions of their learning algorithms using IDEs such as Jupyter Notebooks. However, in an effort to quickly iterate and implement their ideas, they forget to implement design rules such as no code duplication or the DRY principle. While to a certain degree it’s understandable that they may relax their design rules in the early data exploratory or experimentation phases of the project, they are however not necessarily speeding up their experimentation by duplicating functions and avoiding code reusability. Furthermore, the comparison of their experimental results may be flawed due to inconsistent implementations of particular functions. Therefore, I have made it one of my tasks as an intern to create an internal library for data scientists that contains standardized implementations of functions that are commonly used in the data exploratory and experimentation phases of projects, thereby employing design rules such as no code duplication in practice.

The final rule of simple design echoes the sentiment of the YAGNI principle. In trying to predict the future by making certain features more extensible, developers often trap themselves into creating software with a burdensome design. Although conceived with good intentions, trying to incorporate features that are not yet required adds unnecessary weight and complexity to the software. Software requirements are often extremely fluid, so there is very little chance that implementing an unnecessary feature does not become obsolete. Rather than wasting effort in anticipating requirements in advance and developing features that will never be used, the design should follow an incremental, minimal approach. This minimizes complexity, eliminates unnecessary effort, and ultimately prevents any issues that would otherwise result from carrying speculative features. This rule has been reinforced many times throughout each assignment. Although the goal is to keep the review cycles short by only incorporating incremental features, we have often faced the problem of adding a feature that is not yet needed. For example, when working on the word guessing game of assignment 2, we often tried adding the spellchecking service feature at stages far before it was necessary. Had we continued with that implementation, the spellchecking service would have violated many design principles and required multiple modifications to satisfy the three other rules of simple design. By learning to focus on only including minimal features that adhere to the simple rules of design, our application became much more lightweight and maintainable. 

(Richard) Likewise, simply by learning to follow this rule when working on user stories for my internship, I have been able to save myself from multiple headaches. Previously while developing a page to display a report, I would try to add unnecessary features in hopes of making the report more intuitive and flexible. Instead, I created many opportunities for bugs, as our QA team discovered during regression testing. Ultimately, many features were discarded or changed, but this experience nonetheless provided me a great lesson on keeping changes minimal.

(Alan) During my earlier experiences as a software engineering intern I have been guilty of adding unnecessary complexity to a project by ignorantly trying to “think ahead and be proactive” by building future functionality into a class or function that is being developed with a specific purpose in mind, thereby violating design principles such as YAGNI and SRP. Of course, by working with experienced developers and taking a formal software design course, I am now aware of my silly ignorance although it was stemming from good intentions. As a result, while not perfect, I strive to develop concise classes and functions with a single responsibility that implements the required functionality without any extra “bells and whistles”. 

As we have recounted in our personal experiences with code development both in-class and out-off-class, we have first-hand experienced the benefits of the implementation of certain software design principles echoed in Martin Fowler's interpretation of Kent Beck’s four rules of simple design: ensuring the code passes tests, revealing intentions clearly, eliminating duplication, and incorporating only essential elements. By integrating these simple design rules we are able to shape the design of our code to minimize cost and maximize the benefit over the lifetime of the application regardless of programming language or programming paradigm.


2. What design patterns did you use in assignment 3? Discuss.

PLEASE REPLACE THIS WITH YOUR RESPONSE
